P1 MongoDB
Create Mongo DAtaBase -
> use beginnersbook
> db
> show dbs
Insert in db -
> db.user.insert({name:"Chaitanya",age:"30"})
> show dbs
Drop DB -
> db.dropDatabase()
> show dbs
Creating collection -
> use beginnersbookdb
> db.beginnersbook.insert({name:"Chaitanya",age:"30",website:"www.google.com"})
Show Collections -
> show collections
View Collections - 
> db.beginnersbook.find()
Creating Collection manually -  
> db.createCollection("teachers",{capped:true,size:9232768})
Drop Collection -
> db.teachers.drop()
Inserting Multiple Fields  in collection -
> db.beginnersbook.insert({name:"Omkar",age:20,email:"or123@gmail.com",course:[{name:"MonoDB",Duration:7},{name:"Java",Duration:30}]})
> db.beginnersbook.find()
Inserting Multiple Doc  in collection -
> var beginners= [ {"StudentID":1001, "StudentName":"Steve","age":30},{"StudentID":1002, "StudentName":"nege","age":40},{"StudentID":3333, "StudentName":"rick","age":50}]      
 > db.students.insert(beginners);
> db.students.find()
Viewing in JavaScript format -
> db.students.find().forEach(printjson)
> db.students.find({StudentName:"Steve"}).pretty()
Greater and Less Than - 
> db.students.find({"age":{$gt:32}}).pretty()
> db.students.find({"age":{$lt:32}}).pretty()
> db.students.find({"age":{$lte:40}}).pretty()
> db.students.find({"age":{$gte:40}}).pretty()
> db.students.find({"age":{$ne:40}}).pretty()  
Update Doc -
>db.students.update({"StudentName":"Steve"},{$set:{"StudentName":"Jhon"}})
> db.students.find()
Saving Doc -
> db.students.save({ "_id" : ObjectId("5c383475683d8084043ccb08"),"StudentID" : 1001, "StudentName" : "Rocky", "age" : 30 })
> db.students.find()
Delete Doc -
> db.students.remove({"age":50}) 
> db.students.find()  
Particular Columns -       
> db.students.find({},{"_id":0,"StudentID":1})    
Limit And Skip -                 
> db.students.find({"StudentID":{$gt:1000}}).limit(1).skip(1).pretty()    
Sorting -
> db.students.find({},{"_id":0,"StudentID":1}).sort({})    
> db.students.find().sort({})  
Creating Index-
> db.students.createIndex({"StudentName":1})     
Get INDEX -
> db.students.getIndexes()  
Drop index- 
> db.students.dropIndex({"StudentName":1})
Drop All Index -
> db.students.dropIndexes()                               


P2 Simple and Multiple Linear regression

house=read.csv(file.choose(),sep=",",header=T)
reg1=lm(death_rate~hosp_avail,data = house)
summary(reg1)

plot(house$hosp_avail,house$death_rate)
abline(reg1,col="red")
plot(reg1)

house=read.csv(file.choose(),sep=",",header=T)
pairs(~death_rate+doctor_avail+annual_income+density_per_capita,data=house)
housemodel=lm(density_per_capita~death_rate+doctor_avail+hosp_avail+annual_income,data=house)
summary(housemodel)
plot(housemodel)

index=read.csv(file.choose(),sep=",",header = T)
names(index)
pairs(~index+written+language+tech+gk,data=index)
model1=lm(index~.,data=index)
summary(model1)
plot(model1)

index$pred=fitted(model1)
head(index)
index$res=residuals(model1)
head(index)

library(car)
vif(model1)
plot(index$pred,index$res,col="red")

library(car)
ncvTest(model1,~written+language+tech+gk)

shapiro.test(index$res)

library(car)
durbinWatsonTest(model1)

influencePlot(model1)
index=index[-33,]

library("caret")
library("lattice")
library("ggplot2")
index<-read.csv(file.choose(),sep=",",header = T)
summary(index)
data<-createDataPartition(index$empid,p=0.8,list=F)
head(data)
dim(data)

traindata<-index[data,]
testdata<-index[-data,]
dim(traindata)
dim(testdata)

names(traindata)
modeltrain<-lm(index~written+language+tech+gk,data=traindata)
modeltrain$res<-residuals(modeltrain)
RMSEtrain<-sqrt(mean(modeltrain$res**2))
RMSEtrain

testdata$pred<-predict(modeltrain,testdata)
testdata$res<-testdata$index-testdata$pred
RMSEtest<-sqrt(mean(testdata$res**2))
RMSEtest

kfolds<-trainControl(method = "cv",number = 4)
modelkfold<-train(index~written+language+tech+gk,data = index,method="lm",trControl=kfolds)
modelkfold

kfoldsrp<-trainControl(method = "repeatedcv",number = 4,repeats = 5)
modelkfoldsrp<-train(index~written+language+tech+gk,data = index,method="lm",trControl=kfoldsrp)
modelkfoldsrp

kfoldsloocv<-trainControl(method = "LOOCV")
kfoldsloocvmodel<-train(index~written+language+tech+gk,data = index,method="lm",trControl=kfoldsloocv)
kfoldsloocvmodel

    

P3 Logisticreg   

biopsy$ID=NULL
names(biopsy)=c("thick","usize","ushape","adhsn","celsiz","nucl","chrom","nuclus","mit","class")
colSums(is.na(biopsy))
biopsy1=na.omit(biopsy)
set.seed(123)
ind=sample(2,nrow(biopsy1),replace = TRUE,prob = c(0.7,0.3))
train=biopsy1[ind==1,]
test=biopsy1[ind==2,]
str(test)
table(train$class)
table(test$class)
fullfit=glm(class~.,family=binomial,data=train)
summary(fullfit)
exp(coef(fullfit))
train$prob=predict(fullfit,type="response")
train$prob[1:5]
train$class[1:5]
train$predict=rep("benign",474)
train$predict[train$prob>0.5]="malignant"
table(train$predict,train$class)
mean(train$predict==train$class)

test$prob=predict(fullfit,newdata=test,type="response")
test$predict=rep("benign",209)
test$predict[test$prob>0.5]="malignant"
table(test$predict, test$class)
mean(test$predict==test$class)

library(ROCR)
ROCRpred<-prediction(test$prob,test$class)
ROCRpref<-performance(ROCRpred,'tpr','fpr')
plot(ROCRpref,colorize=TRUE)
auc=performance(ROCRpred,'auc')
auc@y.values
reducefit=glm(class~thick+nucl,family=binomial,data=train)
summary(reducefit)
train$prob=predict(reducefit,type="response")
train$predict=rep("benign",474)
train$predict[train$prob>0.5]="malignant"
cf=table(train$predict,train$class)
mean(train$predict==train$class)
library(caret)
sensitivity(cf)
specificity(cf)

test$prob=predict(reducefit,newdata=test,type="response")
test$predict=rep("benign",209) #209 is no.of rows in test dataset
test$predict[test$prob>0.5]="malignant"
table(test$predict,test$class)

mean(test$predict==test$class)   


P4  Hypothesis

"test for normal distribution"
data1<-read.csv(file.choose(),sep = ",",header=T)
shapiro.test(data1$C1)
"Average apple sold in a day are 97"
"H0:mu=97"
"H1:not H0"
"One sample t test"
apple<-read.csv(file.choose(),sep=",",header = T)
summary(apple)
t.test(apple$C1,alternative = "less",mu=97)
"Accept H0"
"The company is assessing the different in salary of males and females."
"Ho: Average time is equal for 2 groups"
"H1: not H0"
"independent t test"
Salary<-read.csv(file.choose(),sep=",",header = T)
summary(Salary)
t.test(Salary$MALES,Salary$FEMALES,alternative = "two.side",var.equal = TRUE)
"Accept H0"
"A survey was done organized to check poverty level"
"H0:Poverty is increased"
"H1:not H0"
"Paried t test"
poverty<-read.csv(file.choose(),sep=",",header = T)
t.test(poverty$X1,poverty$X2,alternative = "greater",paired = T)
"Accept H0"
"Paired t test"
"MIS report:
H0: Average time is equal (before and after)
H1: average time(after) is less than"
"paired t test"
time1<-read.csv(file.choose(),sep=",",header = T)
t.test(time1$time_before,time1$time_after, alternative = "less", paired = T)
"Accept H0"
"To study correlation between 'apptitude' and job_prof
H0:there is no correlation between scores and job proficiency(??=0)
H1: aptitude scores and job 
proficiency are correlated(?? ?? ?? 0)"
"t test for correlated"
cor<-read.csv(file.choose(),sep=",",header = T)
summary(cor)
cor.test(cor$aptitude,cor$job_prof,alternative = "two.sided",method = "pearson")
"Reject H0"
"varience- INDEPENDENT samples t test.xls
H0:??12=??22 and H1:??12 ??? ??22"
"t test for varience"
var<-read.csv(file.choose(),sep=",",header = T)
summary(var)
var.test(var$time_g1,var$time_g2,alternative = "two.sided")
   

P5  Anova

ftest<-read.csv(file.choose(),sep=",",header=T)
var.test(ftest$time_g1,ftest$time_g2,alternative = "two.sided")

"one way anova"
data1<-read.csv(file.choose(),sep = ",",header = T)
names(data1)
summary(data1)
head(data1)
anv<-aov(formula = satindex~dept,data=data1)
summary(anv)
"Pairwise Comparison"
TukeyHSD(anv)

"two way anova"
data2<-read.csv(file.choose(),sep=",",header = T)
names(data2)
summary(data2)
anv1<-aov(formula = satindex~ dept+exp+dept*exp,data = data2)
summary(anv1) 

P6 decision tree

#classification tree
library(MASS)
library(rpart)
data(biopsy)
biopsy=biopsy[,-1] #delete ID
names(biopsy)=c("thick","u.size","u.shape","adhsn","s.size","nucl","chrom","n.nuc","mit","class") #change the feature names
biopsy.v2=na.omit(biopsy)#delete the observations with missing values
set.seed(123)#random number generator
ind=sample(2,nrow(biopsy.v2),replace = TRUE,prob =c(0.7,0.3))
biop.train=biopsy.v2[ind==1,] #the training data set
biop.test=biopsy.v2[ind==2,] #the test data set
str(biop.test[,10])
set.seed(123)
tree.biop=rpart(class~.,method="class",data=biop.train)
print(tree.biop$cptable)
cp=min(tree.biop$cptable[2,])
prune.tree.biop=prune(tree.biop,cp=cp)

library(partykit)
plot(as.party(tree.biop))
plot(as.party(prune.tree.biop))
rparty.test=predict(prune.tree.biop,newdata=biop.test,type="class")
table(rparty.test,biop.test$class)

#regression tree
library(rpart) #classification and regression trees
library(partykit) #treeplots
library(MASS) #breast and pima indian data
#library(randomForest) #random forests
#library(gbm) #gradient boosting
library(caret)
data(biopsy)
biopsy = biopsy[,-1] #delete ID
names(biopsy)=c("thick","u.size","u.shape","adhsn","s.size","nucl","chrom","n.nuc","mit","class") #change the feature names
biopsy.v2=na.omit(biopsy)#delete the observations with missing values
set.seed(123)#random number generator
ind=sample(2,nrow(biopsy.v2),replace = TRUE,prob =c(0.7,0.3))
biop.train=biopsy.v2[ind==1,] #the training data set
biop.test=biopsy.v2[ind==2,] #the test data set
str(biop.test[,10])
set.seed(123)
tree.biop=rpart(adhsn~.,data=biop.train)
print(tree.biop$cptable)
plotcp(tree.biop)
cp=min(tree.biop$cptable[7,])
prune.tree.biop=prune(tree.biop,cp=cp)
plot(as.party(tree.biop))
plot(as.party(prune.tree.biop))
party.biop.test=predict(prune.tree.biop,newdata=biop.test)
rpart.resid=party.biop.test-biop.test$adhsn #calculate residuals
mean(rpart.resid^2) #calculate MSE

P7 Time series forecasting
library(hflights)
library(data.table)
library(forecast)
dt<-data.table(hflights)
dt[,date:=ISOdate(Year, Month, DayofMonth)]
daily<-dt[,list(N=.N,Delays=sum(ArrDelay,na.rm=TRUE),Cancelled=sum(Cancelled),Distance=mean(Distance)),by=date]
str(daily)
nts<-ts(daily$N,frequency = 7)
plot(nts)
#arima
auto.arima(nts)
auto.arima(nts, approximation=FALSE)
#HoltWinters
fit<-HoltWinters(nts)
plot(fit)
library(forecast)
forecast(fit)
f=forecast(HoltWinters(nts),5)
f
plot(f)
#outliers
cts<-ts(daily$Cancelled)
fit<-auto.arima(cts)
auto.arima(cts)
library(tsoutliers)
outliers<-tso(cts, tsmethod='arima', args.tsmethod=list(order=c(1,1,2)))
outliers
plot(outliers)
plot(tso(ts(daily$Cancelled)))

P8 PCA

data("iris")
data_iris <- iris[1:4]
Cov_data <- cov(data_iris)
Eigen_data <- eigen(Cov_data)
Eigen_data$values
PCA_data <- princomp(data_iris ,cor="False")
PCA_data$dev^2
PCA_data$loadings[,1:4]
Eigen_data$vectors
summary(PCA_data)
biplot(PCA_data)
screeplot(PCA_data, type ="lines")                                                                                      